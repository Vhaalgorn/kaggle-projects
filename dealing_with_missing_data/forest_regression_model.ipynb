{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing data\n",
    "\n",
    "In this notebook we will compare different ways to deal with missing data in a data set. Firstly we will import the full data into a dataframe called `X`. We will also import additional data we will use to test the model into `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('./data/train.csv', index_col='Id')\n",
    "X_test = pd.read_csv('./data/test.csv', index_col='Id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a quick inspection of the data we are dealing with using the `shape` property and `head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 80)\n",
      "    MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "Id                                                                    \n",
      "1           60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "2           20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "3           60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "4           70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "5           60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "   LandContour Utilities LotConfig  ... PoolArea PoolQC Fence MiscFeature  \\\n",
      "Id                                  ...                                     \n",
      "1          Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
      "2          Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
      "3          Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
      "4          Lvl    AllPub    Corner  ...        0    NaN   NaN         NaN   \n",
      "5          Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
      "\n",
      "   MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
      "Id                                                             \n",
      "1        0      2    2008        WD         Normal     208500  \n",
      "2        0      5    2007        WD         Normal     181500  \n",
      "3        0      9    2008        WD         Normal     223500  \n",
      "4        0      2    2006        WD        Abnorml     140000  \n",
      "5        0     12    2008        WD         Normal     250000  \n",
      "\n",
      "[5 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 79)\n",
      "      MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "Id                                                                      \n",
      "1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
      "1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
      "1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
      "1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
      "1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
      "\n",
      "     LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \\\n",
      "Id                                    ...                                      \n",
      "1461         Lvl    AllPub    Inside  ...         120        0    NaN  MnPrv   \n",
      "1462         Lvl    AllPub    Corner  ...           0        0    NaN    NaN   \n",
      "1463         Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv   \n",
      "1464         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
      "1465         HLS    AllPub    Inside  ...         144        0    NaN    NaN   \n",
      "\n",
      "     MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n",
      "Id                                                                 \n",
      "1461         NaN       0       6    2010        WD         Normal  \n",
      "1462        Gar2   12500       6    2010        WD         Normal  \n",
      "1463         NaN       0       3    2010        WD         Normal  \n",
      "1464         NaN       0       6    2010        WD         Normal  \n",
      "1465         NaN       0       1    2010        WD         Normal  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_test.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, in this model we will only deal with numerical data, so we want to remove all columns that don't have numbers using `select_dtypes()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.select_dtypes(exclude='object')\n",
    "X_test = X_test.select_dtypes(exclude='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 37)\n",
      "    MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
      "Id                                                                          \n",
      "1           60         65.0     8450            7            5       2003   \n",
      "2           20         80.0     9600            6            8       1976   \n",
      "3           60         68.0    11250            7            5       2001   \n",
      "4           70         60.0     9550            7            5       1915   \n",
      "5           60         84.0    14260            8            5       2000   \n",
      "\n",
      "    YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  WoodDeckSF  \\\n",
      "Id                                                    ...               \n",
      "1           2003       196.0         706           0  ...           0   \n",
      "2           1976         0.0         978           0  ...         298   \n",
      "3           2002       162.0         486           0  ...           0   \n",
      "4           1970         0.0         216           0  ...           0   \n",
      "5           2000       350.0         655           0  ...         192   \n",
      "\n",
      "    OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n",
      "Id                                                                          \n",
      "1            61              0          0            0         0        0   \n",
      "2             0              0          0            0         0        0   \n",
      "3            42              0          0            0         0        0   \n",
      "4            35            272          0            0         0        0   \n",
      "5            84              0          0            0         0        0   \n",
      "\n",
      "    MoSold  YrSold  SalePrice  \n",
      "Id                             \n",
      "1        2    2008     208500  \n",
      "2        5    2007     181500  \n",
      "3        9    2008     223500  \n",
      "4        2    2006     140000  \n",
      "5       12    2008     250000  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 36)\n",
      "      MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
      "Id                                                                            \n",
      "1461          20         80.0    11622            5            6       1961   \n",
      "1462          20         81.0    14267            6            6       1958   \n",
      "1463          60         74.0    13830            5            5       1997   \n",
      "1464          60         78.0     9978            6            6       1998   \n",
      "1465         120         43.0     5005            8            5       1992   \n",
      "\n",
      "      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  GarageArea  \\\n",
      "Id                                                      ...               \n",
      "1461          1961         0.0       468.0       144.0  ...       730.0   \n",
      "1462          1958       108.0       923.0         0.0  ...       312.0   \n",
      "1463          1998         0.0       791.0         0.0  ...       482.0   \n",
      "1464          1998        20.0       602.0         0.0  ...       470.0   \n",
      "1465          1992         0.0       263.0         0.0  ...       506.0   \n",
      "\n",
      "      WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  \\\n",
      "Id                                                                     \n",
      "1461         140            0              0          0          120   \n",
      "1462         393           36              0          0            0   \n",
      "1463         212           34              0          0            0   \n",
      "1464         360           36              0          0            0   \n",
      "1465           0           82              0          0          144   \n",
      "\n",
      "      PoolArea  MiscVal  MoSold  YrSold  \n",
      "Id                                       \n",
      "1461         0        0       6    2010  \n",
      "1462         0    12500       6    2010  \n",
      "1463         0        0       3    2010  \n",
      "1464         0        0       6    2010  \n",
      "1465         0        0       1    2010  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_test.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cleared the data so we only using what we want to use. Now we can check if `X` and `X_test` has any `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotFrontage    259\n",
       "MasVnrArea       8\n",
       "GarageYrBlt     81\n",
       "dtype: int64"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nan_columns=X.isna().sum()[X.isna().sum()>0]\n",
    "X_nan_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotFrontage     227\n",
       "MasVnrArea       15\n",
       "BsmtFinSF1        1\n",
       "BsmtFinSF2        1\n",
       "BsmtUnfSF         1\n",
       "TotalBsmtSF       1\n",
       "BsmtFullBath      2\n",
       "BsmtHalfBath      2\n",
       "GarageYrBlt      78\n",
       "GarageCars        1\n",
       "GarageArea        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_nan_columns=X_test.isna().sum()[X_test.isna().sum()>0]\n",
    "X_test_nan_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the missing variable from `X` and `X_test` we can see that `LotFrontage`, `MasVnrArea` and `GarageYrBlt` are the features with the most `NaN` values. \n",
    "\n",
    "Our target variable is `SalesPrice`. Since this column is not present in either tests earlier we can proceed as normal. If `NaN` values were present in the target variable we would need to removed the whole row.\n",
    "\n",
    "Now, we want to assign the target variable to `y` and remove it from `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['SalePrice'] # assign target variable to `y`\n",
    "X.drop('SalePrice', axis ='columns', inplace= True) # remove column SalePrice from `X`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following variables so far:\n",
    "* `X` and `y`: These data will be used to create the final model once we find it.\n",
    "* `X_test` and `y_test`: These data will be used to test the final model.\n",
    "\n",
    "We will now create the following variables using `X` and `y` and the `train_test_split()` method:\n",
    "* `X_train` and `y_train`: This data will be used to create several models, the data is essentially the control variable in a experiment to find the best model.\n",
    "* `X_val` and `y_val`: This data will be used to validate the models we create to determine which is the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X,y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a function to check the MAE of each model we try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_score(X_train, X_val, y_train, y_val):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train,y_train) # fit model with data\n",
    "    predictions = model.predict(X_val) # create predictions\n",
    "    return mean_absolute_error(y_val, predictions) # estimate mae of predictions based on the known values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using amputation as a method to deal with missing data\n",
    "\n",
    "By completely removing columns that have `NaN` values we can improve the model. However this usually only works when a large number of the values in the column are missing. By removing a whole column we would be removing potentially useful data.\n",
    "\n",
    "The first thing we do is to find the columns that have `NaN` values. We can obtain this using the `index` property of `X_nan_columns`. We can pass the columns to `drop()` method to drope them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_X_train = X_train.drop(X_nan_columns.index, axis=1)\n",
    "reduced_X_valid = X_val.drop(X_nan_columns.index, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `mae_score()`. We will compare the MAE to another model where we used a different method to deal with the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17837.82570776256\n"
     ]
    }
   ],
   "source": [
    "print(mae_score(reduced_X_train, reduced_X_valid, y_train, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with data using imputation method\n",
    "\n",
    "Imputation relates to substituting the `NaN` values with another value. For example, we can calculate the mean value in a column and use it to replace the `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imputing process removed the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>11694.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>774.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>6600.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>308.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>13360.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>69.614017</td>\n",
       "      <td>13265.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>857.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.0</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>13704.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>843.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2006.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0           1        2    3    4       5       6      7       8    9   \\\n",
       "0  20.0   90.000000  11694.0  9.0  5.0  2007.0  2007.0  452.0    48.0  0.0   \n",
       "1  20.0   60.000000   6600.0  5.0  5.0  1962.0  1962.0    0.0     0.0  0.0   \n",
       "2  30.0   80.000000  13360.0  5.0  7.0  1921.0  2006.0    0.0   713.0  0.0   \n",
       "3  20.0   69.614017  13265.0  8.0  5.0  2002.0  2002.0  148.0  1218.0  0.0   \n",
       "4  20.0  118.000000  13704.0  7.0  5.0  2001.0  2002.0  150.0     0.0  0.0   \n",
       "\n",
       "   ...     26     27     28    29   30     31   32   33   34      35  \n",
       "0  ...  774.0    0.0  108.0   0.0  0.0  260.0  0.0  0.0  7.0  2007.0  \n",
       "1  ...  308.0    0.0    0.0   0.0  0.0    0.0  0.0  0.0  8.0  2009.0  \n",
       "2  ...  432.0    0.0    0.0  44.0  0.0    0.0  0.0  0.0  8.0  2009.0  \n",
       "3  ...  857.0  150.0   59.0   0.0  0.0    0.0  0.0  0.0  7.0  2008.0  \n",
       "4  ...  843.0  468.0   81.0   0.0  0.0    0.0  0.0  0.0  1.0  2006.0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.0</td>\n",
       "      <td>69.614017</td>\n",
       "      <td>32668.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>103.481067</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>484.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>9490.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>403.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2006.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.0</td>\n",
       "      <td>69.614017</td>\n",
       "      <td>7015.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>352.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.0</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>10005.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>392.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>505.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0          1        2    3    4       5       6           7       8   \\\n",
       "0   20.0  69.614017  32668.0  6.0  3.0  1957.0  1975.0  103.481067  1219.0   \n",
       "1   50.0  79.000000   9490.0  6.0  7.0  1941.0  1950.0    0.000000   403.0   \n",
       "2   50.0  69.614017   7015.0  5.0  4.0  1950.0  1950.0  161.000000   185.0   \n",
       "3   60.0  83.000000  10005.0  7.0  5.0  1977.0  1977.0  299.000000   392.0   \n",
       "4  160.0  21.000000   1680.0  6.0  5.0  1971.0  1971.0  381.000000     0.0   \n",
       "\n",
       "      9   ...     26     27     28     29   30   31   32   33   34      35  \n",
       "0    0.0  ...  484.0    0.0    0.0  200.0  0.0  0.0  0.0  0.0  3.0  2007.0  \n",
       "1  165.0  ...  240.0    0.0    0.0   32.0  0.0  0.0  0.0  0.0  8.0  2006.0  \n",
       "2    0.0  ...  352.0    0.0    0.0  248.0  0.0  0.0  0.0  0.0  7.0  2009.0  \n",
       "3    0.0  ...  505.0  288.0  117.0    0.0  0.0  0.0  0.0  0.0  3.0  2008.0  \n",
       "4    0.0  ...  264.0    0.0    0.0    0.0  0.0  0.0  0.0  0.0  3.0  2010.0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_X_valid.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put the columns back very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>11694.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>774.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>6600.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>308.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>13360.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>69.614017</td>\n",
       "      <td>13265.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>857.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.0</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>13704.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>843.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2006.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0        20.0    90.000000  11694.0          9.0          5.0     2007.0   \n",
       "1        20.0    60.000000   6600.0          5.0          5.0     1962.0   \n",
       "2        30.0    80.000000  13360.0          5.0          7.0     1921.0   \n",
       "3        20.0    69.614017  13265.0          8.0          5.0     2002.0   \n",
       "4        20.0   118.000000  13704.0          7.0          5.0     2001.0   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  GarageArea  \\\n",
       "0        2007.0       452.0        48.0         0.0  ...       774.0   \n",
       "1        1962.0         0.0         0.0         0.0  ...       308.0   \n",
       "2        2006.0         0.0       713.0         0.0  ...       432.0   \n",
       "3        2002.0       148.0      1218.0         0.0  ...       857.0   \n",
       "4        2002.0       150.0         0.0         0.0  ...       843.0   \n",
       "\n",
       "   WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  \\\n",
       "0         0.0        108.0            0.0        0.0        260.0       0.0   \n",
       "1         0.0          0.0            0.0        0.0          0.0       0.0   \n",
       "2         0.0          0.0           44.0        0.0          0.0       0.0   \n",
       "3       150.0         59.0            0.0        0.0          0.0       0.0   \n",
       "4       468.0         81.0            0.0        0.0          0.0       0.0   \n",
       "\n",
       "   MiscVal  MoSold  YrSold  \n",
       "0      0.0     7.0  2007.0  \n",
       "1      0.0     8.0  2009.0  \n",
       "2      0.0     8.0  2009.0  \n",
       "3      0.0     7.0  2008.0  \n",
       "4      0.0     1.0  2006.0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_X_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the imputed data, we can use `mae_score()` and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18062.894611872147\n"
     ]
    }
   ],
   "source": [
    "print(mae_score(imputed_X_train,imputed_X_valid, y_train, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation performed slightly worse! But why?\n",
    "\n",
    "Sometimes, missing data can tell you some information about the data. For example, the feature `GarageYrBuilt` tells you what year a garage was built, but many houses do not have a garage, so for many houses this feature will have a `NaN` value.\n",
    "\n",
    "With this information we can start to think what we could do next, the know that droping the column performs better, but we are sill droping the column and therefore losing data. We could replace the `NaN` values with 0. This means that we take into count that a house does not have a garage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18053.780251141554\n"
     ]
    }
   ],
   "source": [
    "X_train_nan_0 = X_train.copy()\n",
    "X_train_nan_0['GarageYrBlt'].fillna(value=0, inplace=True)\n",
    "X_train_nan_0 = pd.DataFrame(imputer.transform(X_train_nan_0))\n",
    "X_train_nan_0.columns = X_train.columns\n",
    "\n",
    "X_val_nan_0 = X_val.copy()\n",
    "X_val_nan_0['GarageYrBlt'].fillna(value=0, inplace=True)\n",
    "X_val_nan_0 = pd.DataFrame(imputer.transform(X_val_nan_0))\n",
    "X_val_nan_0.columns = X_val.columns\n",
    "\n",
    "print(mae_score(X_train_nan_0,X_val_nan_0, y_train, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new imputation model performed better `mae = 18053` than the first one `mae = 18062` but not by much. Column drop still performed better `mae = 17837`. We are probably right to fill the `NaN` values of the `GarageYrBlt` with `0`. We can now focus on another column, `LotFrontage`.\n",
    "\n",
    "After some research, Lot frontage is the lot area in front of the house. Not all houses have a front lot. This means that the `NaN` values may be houses that don't have a front lot area. We can try to fill the `NaN` values with `0` to see if there is a improvement in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18050.753778538812\n"
     ]
    }
   ],
   "source": [
    "X_train_fillnan = X_train.copy()\n",
    "X_train_fillnan['GarageYrBlt'].fillna(value=0, inplace=True)\n",
    "X_train_fillnan['LotFrontage'].fillna(value=0, inplace=True)\n",
    "X_train_fillnan = pd.DataFrame(imputer.transform(X_train_fillnan))\n",
    "X_train_fillnan.columns = X_train.columns\n",
    "\n",
    "X_val_fillnan = X_val.copy()\n",
    "X_val_fillnan['GarageYrBlt'].fillna(value=0, inplace=True)\n",
    "X_val_fillnan['LotFrontage'].fillna(value=0, inplace=True)\n",
    "X_val_fillnan = pd.DataFrame(imputer.transform(X_val_fillnan))\n",
    "X_val_fillnan.columns = X_val.columns\n",
    "\n",
    "print(mae_score(X_train_fillnan,X_val_fillnan, y_train, y_val))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very small improvement, in the mae. Now, in this model we are using 36 featured, the amputation method uses 33. This may be a overfitting problem. But to make sure we will also fill NaN of `MasVnrArea`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18017.665970319635\n"
     ]
    }
   ],
   "source": [
    "X_train_fillnan_1= X_train.copy()\n",
    "X_train_fillnan_1['GarageYrBlt'].fillna(value=0, inplace=True)\n",
    "X_train_fillnan_1['LotFrontage'].fillna(value=0, inplace=True)\n",
    "X_train_fillnan_1['MasVnrArea'].fillna(value=0, inplace=True)\n",
    "#X_train_fillnan_1 = pd.DataFrame(imputer.transform(X_train_fillnan_1))\n",
    "#X_train_fillnan_1.columns = X_train.columns\n",
    "\n",
    "X_val_fillnan_1 = X_val.copy()\n",
    "X_val_fillnan_1['GarageYrBlt'].fillna(value=0, inplace=True)\n",
    "X_val_fillnan_1['LotFrontage'].fillna(value=0, inplace=True)\n",
    "X_val_fillnan_1['MasVnrArea'].fillna(value=0, inplace=True)\n",
    "#X_val_fillnan_1 = pd.DataFrame(imputer.transform(X_val_fillnan_1))\n",
    "#X_val_fillnan_1.columns = X_val.columns\n",
    "\n",
    "print(mae_score(X_train_fillnan_1,X_val_fillnan_1, y_train, y_val))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a fairly bigger improvement, `18017.665970319635`. However amputation still performes better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try ising a different strategy for the imputer, we can use the `median` strategy. We will need to create a new imputer for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SimpleImputer(strategy='median')"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_imputer = SimpleImputer(strategy='median')\n",
    "median_imputer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17791.59899543379\n"
     ]
    }
   ],
   "source": [
    "X_train_median_1 = pd.DataFrame(median_imputer.transform(X_train))\n",
    "X_valid_median_1 = pd.DataFrame(median_imputer.transform(X_val))\n",
    "X_train_median_1.columns = X_train.columns\n",
    "X_valid_median_1.columns = X_val.columns\n",
    "\n",
    "print(mae_score(X_train_median_1,X_valid_median_1, y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18128.13852739726\n"
     ]
    }
   ],
   "source": [
    "X_train_median_2 = X_train.copy()\n",
    "X_train_median_2['MasVnrArea'].fillna(value=0, inplace=True)\n",
    "#X_train_median_2['GarageYrBlt'].fillna(value=0, inplace=True)\n",
    "X_train_median_2['LotFrontage'].fillna(value=0, inplace=True)\n",
    "X_train_median_2 = pd.DataFrame(median_imputer.transform(X_train_median_2))\n",
    "X_train_median_2.columns = X_train.columns\n",
    "\n",
    "X_valid_median_2 = X_val.copy()\n",
    "X_valid_median_2['MasVnrArea'].fillna(value=0, inplace=True)\n",
    "#X_valid_median_2['GarageYrBlt'].fillna(value=0, inplace=True)\n",
    "X_valid_median_2['LotFrontage'].fillna(value=0, inplace=True)\n",
    "X_valid_median_2 = pd.DataFrame(median_imputer.transform(X_valid_median_2))\n",
    "X_valid_median_2.columns = X_val.columns\n",
    "X_valid_median_2.isna().any().sum()\n",
    "\n",
    "print(mae_score(X_train_median_2,X_valid_median_2, y_train, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Amputation:                             17837.82570776256\n",
    "* **median method:                          17791.59899543379**\n",
    "* **MasVnrArea:                             17791.59899543379**\n",
    "* MasVnrArea, GarageYrBlt:                17807.438333333328\n",
    "* MasVnrArea, GarageYrBlt, LotFrontage:   18017.665970319635\n",
    "* LotFrontage:                            18128.13852739726\n",
    "* LotFrontage, GarageYrBlt:               18017.665970319635\n",
    "* GarageYrBlt:                            17807.438333333328\n",
    "* MasVnrArea, LotFrontage:                18128.13852739726\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that preprocessing method we want to use. We will use the imputing with `median` strategy. Now that we know this is the best approach we can start to preprocess the `X` and `X_test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_imputer = SimpleImputer(strategy='median')\n",
    "final_imputer.fit(X)\n",
    "final_X_test= pd.DataFrame(final_imputer.transform(X_test), columns=X_test.columns)\n",
    "final_X = pd.DataFrame(final_imputer.transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().any().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a model and fit the preprocessed data into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[450], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(X,y)\n\u001b[1;32m      3\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(final_X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[1;32m    347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 348\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    349\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[1;32m    350\u001b[0m )\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1148\u001b[0m     X,\n\u001b[1;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1152\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[1;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[1;32m    960\u001b[0m             array,\n\u001b[1;32m    961\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    962\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    963\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    964\u001b[0m         )\n\u001b[1;32m    966\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    125\u001b[0m     X,\n\u001b[1;32m    126\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[1;32m    127\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    128\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[1;32m    129\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    130\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model.fit(X,y)\n",
    "predictions = model.predict(final_X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
